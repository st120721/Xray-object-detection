{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of XrayObjectDetection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1elV2VZ0yNQB",
        "outputId": "4c7b33fa-51e9-4de9-b4e1-84edc1ad465d"
      },
      "source": [
        "import os\r\n",
        "import csv\r\n",
        "import numpy as np\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# Module for Google Drive\r\n",
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount('/content/drive')\r\n",
        "%cd './drive/MyDrive/XrayObjectDetection/codes/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "[Errno 2] No such file or directory: './drive/MyDrive/XrayObjectDetection/codes/'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkstEAbq41wl",
        "outputId": "979097da-6903-4db5-e981-362712e4408e"
      },
      "source": [
        "import matplotlib.patches as patches\r\n",
        "from PIL import Image\r\n",
        "from xml.dom.minidom import parse\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "\r\n",
        "\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(f'Current device is: {device}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current device is: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifZNFBNgNoJ2"
      },
      "source": [
        "class XrayDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self, data_path, phase):\r\n",
        "        self.data_path = data_path\r\n",
        "        self.phase = phase\r\n",
        "        self.images_path = os.path.join(self.data_path, self.phase)\r\n",
        "        self.image_names = sorted(os.listdir(self.images_path))\r\n",
        "        self.class_to_idx = {\"Gun\": 1, \"Knife\": 2, \"Wrench\": 3, \"Pliers\": 4}\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.image_names)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        full_image_path = os.path.join(self.images_path, self.image_names[idx])\r\n",
        "        img = Image.open(full_image_path)\r\n",
        "\r\n",
        "        # tensor_transform = torchvision.transforms.ToTensor()\r\n",
        "        # resize the image, batch size ??\r\n",
        "        # VGG16 accepts image of size (3, 224, 224)\r\n",
        "        # resize = torchvision.transforms.Resize(size=224)\r\n",
        "        # transforms = torchvision.transforms.Compose([tensor_transform\r\n",
        "        #                                                      ])\r\n",
        "        # img= transforms(img)\r\n",
        "\r\n",
        "        dom = parse(\"{}/Annotation/{}.xml\".format(self.data_path, self.image_names[idx].split(\".\")[0]))\r\n",
        "        data = dom.documentElement\r\n",
        "\r\n",
        "        objects = data.getElementsByTagName('object')\r\n",
        "        width = data.getElementsByTagName('width')[0].childNodes[0].nodeValue\r\n",
        "        height = data.getElementsByTagName('height')[0].childNodes[0].nodeValue\r\n",
        "        height, width = float(height), float(width)\r\n",
        "        boxes = []\r\n",
        "        labels = []\r\n",
        "        for obj in objects:\r\n",
        "            name = obj.getElementsByTagName('name')[0].childNodes[0].nodeValue\r\n",
        "            bndbox = obj.getElementsByTagName('bndbox')[0]\r\n",
        "            xmin = bndbox.getElementsByTagName('xmin')[0].childNodes[0].nodeValue\r\n",
        "            ymin = bndbox.getElementsByTagName('ymin')[0].childNodes[0].nodeValue\r\n",
        "            xmax = bndbox.getElementsByTagName('xmax')[0].childNodes[0].nodeValue\r\n",
        "            ymax = bndbox.getElementsByTagName('ymax')[0].childNodes[0].nodeValue\r\n",
        "            labels.append(self.class_to_idx[name])\r\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\r\n",
        "\r\n",
        "        H, W = height, width\r\n",
        "\r\n",
        "        \r\n",
        "        o_H, o_W = img.size[0], img.size[1]\r\n",
        "        for idx, bbox in enumerate(boxes):\r\n",
        "            boxes[idx] = self.resize_bbox(bbox, (H, W), (o_H, o_W))\r\n",
        "\r\n",
        "        img = torchvision.transforms.functional.resize(img, (300, 300))\r\n",
        "        img = torchvision.transforms.functional.to_tensor(img)\r\n",
        "        mean, std = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)  \r\n",
        "        img = torchvision.transforms.functional.normalize(img, mean, std)\r\n",
        "       \r\n",
        "        return img, boxes, labels\r\n",
        "\r\n",
        "\r\n",
        "    def resize_bbox(self, bbox, in_size, out_size):\r\n",
        "        y_scale = float(out_size[0]) / in_size[0]\r\n",
        "        x_scale = float(out_size[1]) / in_size[1]\r\n",
        "        xmin = x_scale * float(bbox[0])\r\n",
        "        ymin = y_scale * float(bbox[1])\r\n",
        "        xmax = x_scale * float(bbox[2])\r\n",
        "        ymax = y_scale * float(bbox[3])\r\n",
        "        return [xmin, ymin, xmax, ymax]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3XoaT4s3ndw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "394c21c3-9a6e-4295-c52c-6d9bcb7ef74b"
      },
      "source": [
        "from model.xray_dataloder import XrayDataset\r\n",
        "\r\n",
        "data_path=\"./dataset\"\r\n",
        "# def collate_fn(batch):\r\n",
        "#     return (zip(*batch))\r\n",
        "batch_size= 1\r\n",
        "train_dataset = XrayDataset(data_path, \"train\")\r\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, collate_fn=lambda batch:zip(*batch))\r\n",
        "\r\n",
        "val_dataset = XrayDataset(data_path, \"val\")\r\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, pin_memory=True, collate_fn=lambda batch:zip(*batch))\r\n",
        "\r\n",
        "print(len(train_dataloader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-217ef094fb71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxray_dataloder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXrayDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./dataset\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# def collate_fn(batch):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     return (zip(*batch))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLPeLJoU5Flx"
      },
      "source": [
        "idx2class = {1:\"Gun\", 2:\"Knife\", 3:\"Wrench\"}\r\n",
        "\r\n",
        "img, boxes, labels = next(iter(train_dataloader))\r\n",
        "\r\n",
        "fig, ax = plt.subplots(1, 5, figsize=(20, 10))\r\n",
        "for i in range(5):\r\n",
        "    ax[i].imshow(img[i].permute(1, 2, 0))\r\n",
        "    for j, box in enumerate(boxes[i]):\r\n",
        "       for label in labels[i]:\r\n",
        "          class_name = idx2class[label]\r\n",
        "          xy = (box[0], box[1])\r\n",
        "          w = float(box[2]) - xy[0] \r\n",
        "          h = float(box[3]) - xy[1] \r\n",
        "          rect = plt.Rectangle(xy, w, h, fill=False, edgecolor = 'red',linewidth=1)\r\n",
        "          ax[i].add_patch(rect)\r\n",
        "          ax[i].text(xy[0], xy[1]-5, class_name, c='r')\r\n",
        "\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swCgNNdzpT5G"
      },
      "source": [
        "from model.model import *\r\n",
        "\r\n",
        "n_classes=4\r\n",
        "# path_pretrained_state_dict=\"../best_vgg_state_dict.pth\"\r\n",
        "model = SSD300(n_classes=n_classes,path_pretrained_state_dict=path_pretrained_state_dict)\r\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v19tl_4TmVnS"
      },
      "source": [
        "## Training and Validating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1HS41urHaxs"
      },
      "source": [
        "def train(model, criterion, optimizer, images, bboxes, labels):\r\n",
        "\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    images = torch.stack(images, 0).to(device)\r\n",
        "    bboxes = [torch.tensor(b).to(device)/300 for b in bboxes]\r\n",
        "    labels = [torch.tensor(l).to(device) for l in labels]\r\n",
        "    \r\n",
        "    # images, a tensor of dimensions (N, 3, 300, 300)\r\n",
        "    pred_locs, pred_scores = model(images) \r\n",
        "    # object bounding boxes in boundary coordinates, a list of N tensors\r\n",
        "    # object labels, a list of N tensors\r\n",
        "    loss = criterion(pred_locs, pred_scores, bboxes, labels) \r\n",
        "    \r\n",
        "    # clear gradient and perform backprop\r\n",
        "    optimizer.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "\r\n",
        "    # update model\r\n",
        "    optimizer.step()\r\n",
        "    return loss\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llML6s9nhf0d"
      },
      "source": [
        "def validate(model, criterion, images, bboxes, labels):\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        images = torch.stack(images, 0).to(device)\r\n",
        "        bboxes = [torch.tensor(b).to(device)/300 for b in bboxes]\r\n",
        "        labels = [torch.tensor(l).to(device) for l in labels]\r\n",
        "\r\n",
        "        pred_locs, pred_scores = model(images)\r\n",
        "        det_boxes_batch, det_labels_batch, det_scores_batch = model.detect_objects(pred_locs, pred_scores,\r\n",
        "                                                                                       min_score=0.01, max_overlap=0.45,\r\n",
        "                                                                                       top_k=200)\r\n",
        "        loss = criterion(pred_locs, pred_scores, bboxes, labels)\r\n",
        "\r\n",
        "    return loss, det_boxes_batch, det_labels_batch, det_scores_batch\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QZmNp_ju5YK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPw-EBd6nQAy"
      },
      "source": [
        "\r\n",
        "\r\n",
        "max_epoch = 50\r\n",
        "save_stride = 10\r\n",
        "print_stride = 1\r\n",
        "max_val_mAP=0\r\n",
        "\r\n",
        "version = 'original_ssd_pretrained'\r\n",
        "model_dir = '../results'\r\n",
        "tmp_path = '../results/checkpoint_{}.pth'.format(version)\r\n",
        "\r\n",
        "\r\n",
        "# opitmizer\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\r\n",
        "# scheduler \r\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=2)\r\n",
        "# loss function\r\n",
        "# criterion = multi_box_loss.MultiBoxLoss(priors_cxcy=model.priors_cxcy).to(device)\r\n",
        "criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy).to(device)\r\n",
        "\r\n",
        "train_losses = []\r\n",
        "val_losses = []\r\n",
        "val_mAPs = []\r\n",
        "\r\n",
        "for epoch in tqdm(range(max_epoch)):        \r\n",
        "    ###Train Phase\r\n",
        "\r\n",
        "    # Initialize Loss and Accuracy\r\n",
        "    train_loss = 0.0\r\n",
        "    \r\n",
        "    # Load the saved MODEL AND OPTIMIZER after evaluation.\r\n",
        "    if epoch >= 0:\r\n",
        "        checkpoint = torch.load(tmp_path)\r\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\r\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\r\n",
        "\r\n",
        "    # Iterate over the train_dataloader\r\n",
        "    with tqdm(total=len(train_dataloader)) as pbar:\r\n",
        "        for idx, (images, bboxes, labels) in enumerate(train_dataloader):\r\n",
        "\r\n",
        "          \r\n",
        "            curr_loss = train(model, criterion, optimizer, images, bboxes, labels)\r\n",
        "            train_loss += curr_loss / len(train_dataloader)\r\n",
        "            pbar.update(1)\r\n",
        "\r\n",
        "    \r\n",
        "    checkpoint = {\r\n",
        "        'model' : SSD300(n_classes=n_classes,path_pretrained_state_dict=path_pretrained_state_dict),\r\n",
        "        'epoch' : epoch+1,\r\n",
        "        'model_state_dict': model.state_dict(),\r\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\r\n",
        "    }\r\n",
        "    \r\n",
        "    \r\n",
        "    torch.save(checkpoint, tmp_path)\r\n",
        "    # torch.save(checkpoint, os.path.join(model_dir, 'model_recent.pth'))\r\n",
        "\r\n",
        "    ### Validation Phase\r\n",
        "    # Initialize Loss and Accuracy\r\n",
        "    val_loss = 0.0\r\n",
        "\r\n",
        "    det_boxes = list()\r\n",
        "    det_labels = list()\r\n",
        "    det_scores = list()\r\n",
        "    true_boxes = list()\r\n",
        "    true_labels = list()\r\n",
        "\r\n",
        "    # Iterate over the val_dataloader\r\n",
        "    with tqdm(total=len(val_dataloader)) as pbar:\r\n",
        "        \r\n",
        "        for idx, (images, bboxes, labels) in enumerate(val_dataloader):\r\n",
        "            curr_loss, det_boxes_batch, det_labels_batch, det_scores_batch = validate(model, criterion, images, bboxes, labels)\r\n",
        "            val_loss += curr_loss / len(val_dataloader)\r\n",
        "   \r\n",
        "    \r\n",
        "            bboxes = [torch.tensor(b).to(device)/300 for b in bboxes]\r\n",
        "            labels = [torch.tensor(l).to(device) for l in labels]\r\n",
        "           \r\n",
        "            det_boxes.extend(det_boxes_batch)\r\n",
        "            det_labels.extend(det_labels_batch)\r\n",
        "            det_scores.extend(det_scores_batch)\r\n",
        "            true_boxes.extend(bboxes)\r\n",
        "            true_labels.extend(labels)\r\n",
        "            \r\n",
        "            pbar.update(1)\r\n",
        "\r\n",
        "    val_APs, val_mAP = calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels,device)\r\n",
        "\r\n",
        "    if max_val_mAP < val_mAP:\r\n",
        "          torch.save(checkpoint, os.path.join(model_dir, 'best_model_{}.pth'.format(version)))\r\n",
        "          max_val_mAP = val_mAP\r\n",
        "          print(f'new min val loss: {max_val_mAP}')\r\n",
        "\r\n",
        "  \r\n",
        "    if (epoch) % print_stride == 0:\r\n",
        "       print(f'Epoch: {epoch+1} Training loss: {train_loss} Validation loss: {val_loss}')   \r\n",
        "       print(f'mAP of the validation set {val_mAP}: \\n {val_APs}')\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "    train_losses.append(train_loss.cpu().detach().numpy())\r\n",
        "    val_losses.append(val_loss.cpu().detach().numpy())\r\n",
        "    val_mAPs.append(val_mAP)\r\n",
        "   \r\n",
        "    scheduler.step(val_loss)\r\n",
        "\r\n",
        "torch.save(train_losses, os.path.join(model_dir, 'train_losses_{}.pth'.format(version)))\r\n",
        "torch.save(val_losses, os.path.join(model_dir, 'val_losses_{}.pth'.format(version)))\r\n",
        "torch.save(val_mAPs, os.path.join(model_dir, 'val_mAPs_{}.pth'.format(version)))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKslGLXeOq8c"
      },
      "source": [
        "torch.save(train_losses, os.path.join(model_dir, 'train_losses_{}.pth'.format(version)))\r\n",
        "torch.save(val_losses, os.path.join(model_dir, 'val_losses_{}.pth'.format(version)))\r\n",
        "torch.save(val_mAPs, os.path.join(model_dir, 'val_mAPs_{}.pth'.format(version)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f9hCmMsGQDK"
      },
      "source": [
        "# plt.plot(train_losses)\r\n",
        "# plt.plot(val_losses)\r\n",
        "epochs=np.arange(len(val_mAPs))+1\r\n",
        "plt.plot(epochs,val_mAPs,)\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.ylabel('map')\r\n",
        "plt.title('mean of average precision')\r\n",
        "plt.savefig(os.path.join(model_dir, 'map_{}.png'.format(version)))\r\n",
        "plt.show()\r\n",
        "#plt.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LC8pj-SGPz0"
      },
      "source": [
        "plt.plot(epochs,train_losses,label=\"train losses\")\r\n",
        "plt.plot(epochs,val_losses,label=\"validation losses\")\r\n",
        "plt.legend()\r\n",
        "plt.savefig(os.path.join(model_dir,'train_losses_{}.png'.format(version)))\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}